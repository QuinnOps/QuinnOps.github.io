# 这是一篇关于安装k8s的文章

#### 一、环境准备：

三台机器，配置至少2c2g 系统centos7

IP： 192.168.31.202（master）

​		192.168.31.203

​		192.168.31.204

所有机器禁用防火墙、selinux、swap

```shell
#关闭防火墙
systemctl stop firewalld & systemctl disable firewalld
#关闭 selinux
setenforce 0
vi /etc/selinux/config
SELINUX=disabled
# 关闭 swap
swapoff -a
vi /etc/fstab，注释掉swap
```

更换CentOS YUM源为阿里云yum源

```shell
# 安装wget
yum install wget -y
# 备份
mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup
# 获取阿里云yum源
wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
# 获取阿里云epel源
wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo
# 清理缓存并创建新的缓存
yum clean all && yum makecache
# 系统更新
yum update -y
```

进行时间同步，并确认时间同步成功

```shell
timedatectl
timedatectl set-ntp true
```

#### 二、安装docker：

需要在每台机器上安装 Docker，我这里安装的是 docker-ce-19.03.4

```
# 安装 Docker CE
# 设置仓库
# 安装所需包
yum install -y yum-utils \
    device-mapper-persistent-data \
    lvm2

# 新增 Docker 仓库,速度慢的可以换阿里云的源。
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
# 阿里云源地址
# yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

# 安装 Docker CE.
yum install -y containerd.io-1.2.10 \
    docker-ce-19.03.4 \
    docker-ce-cli-19.03.4

# 启动 Docker 并添加开机启动
systemctl start docker
systemctl enable docker
```

修改 Cgroup Driver

需要将Docker 的 Cgroup Driver 修改为 systemd，不然在为Kubernetes 集群添加节点时会报如下错误：

```
# 执行 kubeadm join 的 WARNING 信息
[WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kubernetes.io/docs/setup/cri/
```

目前 Docker 的 Cgroup Driver 看起来应该是这样的：

```
$ docker info|grep "Cgroup Driver"
  Cgroup Driver: cgroupfs
```

需要将这个值修改为 systemd ，同时我将registry替换成国内的一些仓库地址，以免直接在官方仓库拉取镜像会很慢，操作如下：

<!--注意缩进-->，直接复制的缩进可能有问题，请确保缩进为正确的 Json 格式；如果 Docker 重启后查看状态不正常，大概率是此文件缩进有问题，Json格式的缩进自己了解一下。

```
# Setup daemon.
cat > /etc/docker/daemon.json <<EOF
{
    "exec-opts": ["native.cgroupdriver=systemd"],
    "log-driver": "json-file",
    "log-opts": {
    "max-size": "100m"
    },
    "storage-driver": "overlay2",
    "registry-mirrors":[
        "https://kfwkfulq.mirror.aliyuncs.com",
        "https://2lqq34jg.mirror.aliyuncs.com",
        "https://pee6w651.mirror.aliyuncs.com",
        "http://hub-mirror.c.163.com",
        "https://docker.mirrors.ustc.edu.cn",
        "https://registry.docker-cn.com"
    ]
}
EOF

mkdir -p /etc/systemd/system/docker.service.d

# Restart docker.
systemctl daemon-reload
systemctl restart docker
```

#### 三、安装 kubeadm、kubelet 和 kubectl

安装准备：

需要在每台机器上安装以下的软件包：

- kubeadm：用来初始化集群的指令。
- kubelet：在集群中的每个节点上用来启动 pod 和容器等。
- kubectl：用来与集群通信的命令行工具（Worker 节点可以不装，但是装了，不影响什么）。

```
# 配置K8S的yum源
# 这部分用是阿里云的源，如果可以访问Google，则建议用官方的源
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg       http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

# 官方源配置如下
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
```

开始安装

安装指定版本 kubelet、 kubeadm 、kubectl， 这里选择当前较新的稳定版 Kubernetes 1.17.3，如果选择的版本不一样，在执行集群初始化的时候，注意 –kubernetes-version 的值。

```
# 增加配置
cat <<EOF > /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward=1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
# 加载
sysctl --system

# 安装
yum install -y kubelet-1.17.3 kubeadm-1.17.3 kubectl-1.17.3 --disableexcludes=kubernetes

# 启动并设置 kubelet 开机启动
systemctl start kubelet
systemctl enable --now kubelet
```

<!--如果此时执行--> systemctl status kubelet 命令，系统日志将得到 kubelet 启动失败的错误提示，请忽略此错误，因为必须完成后续步骤中 kubeadm init 的操作，kubelet 才能正常启动

#### 使用 Kubeadm 创建集群

初始化 Control-plane/Master 节点

在第一台 Master 上执行初始化，执行初始化使用 kubeadm init 命令。初始化首先会执行一系列的运行前检查来确保机器满足运行 Kubernetes 的条件，这些检查会抛出警告并在发现错误的时候终止整个初始化进程。 然后 kubeadm init 会下载并安装集群的 Control-plane 组件。

在初始化之前，需要先设置一下 hosts 解析，为了避免可能出现的问题，后面的 Worker 节点我也进行了同样的操作。注意按照你的实际情况修改Master节点的IP，并且注意 APISERVER_NAME 的值，如果你将这个 apiserver 名称设置为别的值，下面初始化时候的 –control-plane-endpoint 的值保持一致。

<!--提示-->：为了使 Kubernetes 集群高可用，建议给集群的控制节点配置负载均衡器，如 HAproxy + Keepalived 或 Nginx，云上可以使用公有云的负载均衡器，然后在以下部分设置 MASTER_IP 和 APISERVER_NAME 为负载均衡器的地址（IP:6443） 和域名。

```
# 设置hosts
echo "127.0.0.1 $(hostname)" >> /etc/hosts
export MASTER_IP=192.168.115.49
export APISERVER_NAME=kuber4s.api
echo "${MASTER_IP} ${APISERVER_NAME}" >> /etc/hosts
```

<!--友情提示-->：

截止2020年01月29日，官方文档声明了使用 kubeadm 初始化 master 时，–config 这个参数是实验性质的，所以就不用了；我们用其他参数一样可以完成 master 的初始化。

```
--config string   kubeadm 配置文件。 警告：配置文件的使用是试验性的。
```

下面有不带注释的初始化命令，建议先查看带注释的每个参数对应的意义，确保与你的当前配置的环境是一致的，然后再执行初始化操作，避免踩雷。

```
# 初始化 Control-plane/Master 节点
kubeadm init \
    --apiserver-advertise-address 0.0.0.0 \
    # API 服务器所公布的其正在监听的 IP 地址,指定“0.0.0.0”以使用默认网络接口的地址
    # 切记只可以是内网IP，不能是外网IP，如果有多网卡，可以使用此选项指定某个网卡
    --apiserver-bind-port 6443 \
    # API 服务器绑定的端口,默认 6443
    --cert-dir /etc/kubernetes/pki \
    # 保存和存储证书的路径，默认值："/etc/kubernetes/pki"
    --control-plane-endpoint kuber4s.api \
    # 为控制平面指定一个稳定的 IP 地址或 DNS 名称,
    # 这里指定的 kuber4s.api 已经在 /etc/hosts 配置解析为本机IP
    --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \
    # 选择用于拉取Control-plane的镜像的容器仓库，默认值："k8s.gcr.io"
    # 因 Google被墙，这里选择国内仓库
    --kubernetes-version 1.17.3 \
    # 为Control-plane选择一个特定的 Kubernetes 版本， 默认值："stable-1"
    --node-name master01 \
    #  指定节点的名称,不指定的话为主机hostname，默认可以不指定
    --pod-network-cidr 10.10.0.0/16 \
    # 指定pod的IP地址范围
    --service-cidr 10.20.0.0/16 \
    # 指定Service的VIP地址范围
    --service-dns-domain cluster.local \
    # 为Service另外指定域名，默认"cluster.local"
    --upload-certs
    # 将 Control-plane 证书上传到 kubeadm-certs Secret
```

不带注释的内容如下，如果初始化超时，可以修改DNS为8.8.8.8后重启网络服务再次尝试。

```
kubeadm init \
 --apiserver-advertise-address 0.0.0.0 \
 --apiserver-bind-port 6443 \
 --cert-dir /etc/kubernetes/pki \
 --control-plane-endpoint kuber4s.api \
 --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers \
 --kubernetes-version 1.17.3 \
 --pod-network-cidr 10.10.0.0/16 \
 --service-cidr 10.20.0.0/16 \
 --service-dns-domain cluster.local \
 --upload-certs
```

接下来这个过程有点漫长（初始化会下载镜像、创建配置文件、启动容器等操作），耐心等待，你也可以执行 tailf /var/log/messages 来实时查看系统日志，观察 Master 的初始化进展，期间碰到一些报错不要紧张，可能只是暂时的错误，等待最终反馈的结果即可。

如果初始化最终成功执行，你将看到如下信息：

```
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join kuber4s.api:6443 --token 0j287q.jw9zfjxud8w85tis \
    --discovery-token-ca-cert-hash sha256:5e8bcad5ec97c1025e8044f4b8fd0a4514ecda4bac2b3944f7f39ccae9e4921f \
    --control-plane --certificate-key 528b0b9f2861f8f02dfd4a59fc54ad21e42a7dea4dc5552ac24d9c650c5d4d80

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join kuber4s.api:6443 --token 0j287q.jw9zfjxud8w85tis \
    --discovery-token-ca-cert-hash sha256:5e8bcad5ec97c1025e8044f4b8fd0a4514ecda4bac2b3944f7f39ccae9e4921f
```

为普通用户添加 kubectl 运行权限，命令内容在初始化成功后的输出内容中可以看到。

```
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

建议root用户也进行以上操作，作者使用的是root用户执行的初始化操作，然后在操作完成后查看集群状态的时候，出现如下错误：。

```
The connection to the server localhost:8080 was refused - did you specify the right host or port?
```

这时候请备份好 kubeadm init 输出中的 kubeadm join 命令，因为将会需要这个命令来给集群添加节点。

<!--提示-->：令牌是主节点和新添加的节点之间进行相互身份验证的，因此请确保其安全。任何人只要知道了这些令牌，就可以随便给您的集群添加节点。 你可以使用 kubeadm token 命令来查看、创建和删除这类令牌。























添加k8s源（可以科学上网跳过）

```
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
```

安装ipset ipvsadm

```shell
yum install -y ipset ipvsadm
```

安装docker并自启

```
yum install docker
systemctl start docker & systemctl enable docker
```

配置docker源

```
cat >>/etc/docker/daemon.json<<"EOF"
{
"registry-mirrors": ["http://hub-mirror.c.163.com"]
}
EOF

systemctl restart docker
```

安装三件套，设置kubelet自启

```shell
yum install -y kubectl-1.13.0 kubeadm-1.13.0 kubectl-1.13.0

systemctl enable kubelet
```

kube-proxy开启ipvs

```
cat >>/etc/sysctl.d/k8s.conf<<"EOF"
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
modprobe br_netfilter
EOF

sysctl -p /etc/sysctl.d/k8s.conf

cat > /etc/sysconfig/modules/ipvs.modules <<EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF

chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep -e ip_vs -e nf_conntrack_ipv4
```

修改hostname

```
hostnamectl set-hostname XXXX
```

至此，三台机器相同的配置完成



下面初始化Master节点（最后参数是指定阿里云镜像仓库）

```
kubeadm init --kubernetes-version=v1.13.0 --apiserver-advertise-address=192.168.31.202 --pod-network-cidr=192.168.0.0/16 --service-cidr=172.16.0.0/16 --image-repository registry.aliyuncs.com/google_containers
```

如果出现timeout是因为docker无法自动获取到images导致的，可以两种方式解决

​		A、手动去pull并修改标签

​		B、修改docker源的方式来解决

手动pull images

```
docker pull mirrorgooglecontainers/kube-apiserver:v1.13.0
docker pull mirrorgooglecontainers/kube-controller-manager:v1.13.0
docker pull mirrorgooglecontainers/kube-scheduler:v1.13.0
docker pull mirrorgooglecontainers/kube-proxy:v1.13.0
docker pull mirrorgooglecontainers/pause:3.1
docker pull mirrorgooglecontainers/etcd:3.2.24
docker pull coredns/coredns:1.2.6
docker tag mirrorgooglecontainers/kube-proxy:v1.13.0  k8s.gcr.io/kube-proxy:v1.13.0
docker tag mirrorgooglecontainers/kube-scheduler:v1.13.0 k8s.gcr.io/kube-scheduler:v1.13.0
docker tag mirrorgooglecontainers/kube-apiserver:v1.13.0 k8s.gcr.io/kube-apiserver:v1.13.0
docker tag mirrorgooglecontainers/kube-controller-manager:v1.13.0 k8s.gcr.io/kube-controller-manager:v1.13.0
docker tag mirrorgooglecontainers/etcd:3.2.24  k8s.gcr.io/etcd:3.2.24
docker tag coredns/coredns:1.2.6 k8s.gcr.io/coredns:1.2.6
docker tag mirrorgooglecontainers/pause:3.1  k8s.gcr.io/pause:3.1
```

初始化成功后会生成worker加入集群的指令  在worker节点上执行即可

```
kubeadm join 192.168.31.202:6443 --token f33uwo.4qu4khi3nn6wuon2 --discovery-token-ca-cert-hash sha256:758c34bd065b6fb3c3ae3840d0cf6667204673c5cf6ce9391567120957e8e201
```

执行如下指令

```
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

这时候输入kubectl get cs 就可以查看服务的运行情况，确保都在正常运行

安装network addon(采用官方推荐的flannel方案，其中pod网段必须与之前init的时候设置的pod-network-cidr一致)

```
wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml$ kubectl apply -f kube-flannel.yml
```

将worker join至master 

至此集群已搭建完毕，这时候可以通过kubectl get nodes来查看各节点运行情况
